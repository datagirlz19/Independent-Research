{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datagirlz19/Independent-Research/blob/main/Sample_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SeOmYbG1f-g"
      },
      "source": [
        "Code was taken from: https://blog.jovian.ai/generating-art-with-gans-352ceef3d51f \n",
        "\n",
        "It is a project that does the same thing that I am testing, but modifications needed to be made. \n",
        "\n",
        "While I use this code for only one artist at a time, the things I input into the dataset are to prove a point, not to destroy a data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzZuVmVd1e9y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpK3ND-cD--k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets.folder import default_loader\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAyFahp-ILfP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/MyDrive/DataSets/DataSets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc9mCH000QnW"
      },
      "outputs": [],
      "source": [
        "# https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
        "\n",
        "batch_size = 128 #what is the bath size?- the size of the training set \n",
        "image_size = (64,64) #We need to resize each image\n",
        "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) #what are the stats - a tuple of two tuples "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqg_V2FVOQs3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        ".compose(transformations)\n",
        "Composes several transforms together. This transform does not support torchscript.\n",
        "Please, see the note below.\n",
        "\n",
        "Args:\n",
        "    transforms (list of Transform objects): list of transforms to compose.\n",
        "'''\n",
        "#resizing the images so that they can be used in the model \n",
        "#normalizing them so that they look like squares that are relaticely the same size \n",
        "\n",
        "transform_ds = transforms.Compose([transforms.Resize(image_size),  \n",
        "#                                    transforms.RandomCrop(32, padding=2),\n",
        "#                                    transforms.RandomHorizontalFlip(),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize(*stats)\n",
        "                                   ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEj38B0hymNh"
      },
      "outputs": [],
      "source": [
        "train_ds = torchvision.datasets.ImageFolder(root=\"/content/drive/MyDrive/DataSets\",\n",
        "                                     transform=transform_ds)\n",
        "\n",
        "\n",
        "#why do we need to shuffle?\n",
        "#What are num_workers? \n",
        "#what is pin_memory? \n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "print(len(train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puReJhTO1Tpc"
      },
      "outputs": [],
      "source": [
        "images,_ = train_ds[1]\n",
        "print(images.size())\n",
        "plt.imshow(images.permute(1,2,0))\n",
        "\n",
        "#why are the images so dark? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp07nmqC31c3"
      },
      "outputs": [],
      "source": [
        "#creating a function that takes the tensors as input \n",
        "def denorm(img_tensors):\n",
        "    return img_tensors * stats[1][0] + stats[0][0] #what do the stats do - accessing a speific vectors coordinates "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tCa1K8e311m"
      },
      "outputs": [],
      "source": [
        "#creating two functions \n",
        "def show_images(images, nmax=64): #default to the size of the image 64px\n",
        "    fig, ax = plt.subplots(figsize=(8, 8)) #what do subplots do? - We are resizing to get an 8x8 matrix\n",
        "    ax.set_xticks([]); ax.set_yticks([]) #what do xticks and yticks do? \n",
        "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
        "#puts the original images in a grid \n",
        "def show_batch(dl, nmax=64): #max size is 64 \n",
        "    for images, _ in dl:\n",
        "        show_images(images, nmax)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddfL6vke32DE"
      },
      "outputs": [],
      "source": [
        "show_batch(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2TE9jU832Mj"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kdfP1232Vj"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqh03K5Q32e8"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcuRYNfapPrG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        ".Sequential\n",
        "A sequential container.\n",
        "Modules will be added to it in the order they are passed in the\n",
        "constructor. Alternatively, an OrderedDict of modules can be\n",
        "passed in. The forward() method of Sequential accepts any\n",
        "input and forwards it to the first module it contains. It then\n",
        "\"chains\" outputs to inputs sequentially for each subsequent module,\n",
        "finally returning the output of the last module.\n",
        "\n",
        "The value a Sequential provides over manually calling a sequence\n",
        "of modules is that it allows treating the whole container as a\n",
        "single module, such that performing a transformation on the\n",
        "Sequential applies to each of the modules it stores (which are\n",
        "each a registered submodule of the Sequential).\n",
        "\n",
        "What's the difference between a Sequential and a\n",
        "torch.nn.ModuleList? A ModuleList is exactly what it\n",
        "sounds like--a list for storing Module s! On the other hand,\n",
        "the layers in a Sequential are connected in a cascading way.\n",
        "'''\n",
        "#nn comes from tensor flow for neural networks\n",
        "discriminator = nn.Sequential(\n",
        "    # in: 3 x 64 x 64\n",
        "  \n",
        "    #Applies a 2D convolution over an input signal composed of several input planes.\n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 64 x 32 x 32\n",
        "\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 128 x 16 x 16\n",
        "\n",
        "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 256 x 8 x 8\n",
        "\n",
        "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 512 x 4 x 4\n",
        "\n",
        "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    # out: 1 x 1 x 1\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Sigmoid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbHoQg9qu_OU"
      },
      "outputs": [],
      "source": [
        "discriminator = to_device(discriminator, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xQc83HFvAXV"
      },
      "outputs": [],
      "source": [
        "latent_size = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQyIea00vgYC"
      },
      "outputs": [],
      "source": [
        "generator = nn.Sequential(\n",
        "    # in: latent_size x 1 x 1\n",
        "\n",
        "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(True),\n",
        "    # out: 512 x 4 x 4\n",
        "\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(True),\n",
        "    # out: 256 x 8 x 8\n",
        "\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(True),\n",
        "    # out: 128 x 16 x 16\n",
        "\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(True),\n",
        "    # out: 64 x 32 x 32\n",
        "\n",
        "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.Tanh()\n",
        "    # out: 3 x 64 x 64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA0tud1kvgts"
      },
      "outputs": [],
      "source": [
        "xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\n",
        "fake_images = generator(xb)\n",
        "print(fake_images.shape)\n",
        "show_images(fake_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmjxHUuuvg3t"
      },
      "outputs": [],
      "source": [
        "generator = to_device(generator, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5sHQJ6svwdw"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(real_images, opt_d):\n",
        "    # Clear discriminator gradients\n",
        "    opt_d.zero_grad()\n",
        "\n",
        "    # Pass real images through discriminator\n",
        "    real_preds = discriminator(real_images)\n",
        "    real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
        "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
        "    real_score = torch.mean(real_preds).item()\n",
        "    \n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "    fake_images = generator(latent)\n",
        "\n",
        "    # Pass fake images through discriminator\n",
        "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
        "    fake_preds = discriminator(fake_images)\n",
        "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
        "    fake_score = torch.mean(fake_preds).item()\n",
        "\n",
        "    # Update discriminator weights\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    opt_d.step()\n",
        "    return loss.item(), real_score, fake_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kGS522upP3E"
      },
      "outputs": [],
      "source": [
        "def train_generator(opt_g):\n",
        "    # Clear generator gradients\n",
        "    opt_g.zero_grad()\n",
        "    \n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "    fake_images = generator(latent)\n",
        "    \n",
        "    # Try to fool the discriminator\n",
        "    preds = discriminator(fake_images)\n",
        "    targets = torch.ones(batch_size, 1, device=device)\n",
        "    loss = F.binary_cross_entropy(preds, targets)\n",
        "    \n",
        "    # Update generator weights\n",
        "    loss.backward()\n",
        "    opt_g.step()\n",
        "    \n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSbdCUfIpQCA"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TYD-7YGpQML"
      },
      "outputs": [],
      "source": [
        "sample_dir = 'generated'\n",
        "os.makedirs(sample_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIANZEyUpQW1"
      },
      "outputs": [],
      "source": [
        "def save_samples(index, latent_tensors, show=True):\n",
        "    fake_images = generator(latent_tensors)\n",
        "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
        "    print('Saving', fake_fname)\n",
        "    if show:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-k93rXdpQg_"
      },
      "outputs": [],
      "source": [
        "#fixed_latent = torch.randn(64, any, 1, 1, device=device)\n",
        "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29yEM7hKHiZ5"
      },
      "outputs": [],
      "source": [
        "save_samples(0, fixed_latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwJ_IYUXHikw"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovMLmCBYHisU"
      },
      "outputs": [],
      "source": [
        "def fit(epochs, lr, start_idx=1):\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Losses & scores\n",
        "    losses_g = []\n",
        "    losses_d = []\n",
        "    real_scores = []\n",
        "    fake_scores = []\n",
        "    \n",
        "    # Create optimizers\n",
        "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for real_images, _ in tqdm(train_dl):\n",
        "            # Train discriminator\n",
        "            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
        "            # Train generator\n",
        "            loss_g = train_generator(opt_g)\n",
        "            \n",
        "        # Record losses & scores\n",
        "        losses_g.append(loss_g)\n",
        "        losses_d.append(loss_d)\n",
        "        real_scores.append(real_score)\n",
        "        fake_scores.append(fake_score)\n",
        "        \n",
        "        # Log losses & scores (last batch)\n",
        "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
        "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
        "    \n",
        "        # Save generated images\n",
        "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
        "    \n",
        "    return losses_g, losses_d, real_scores, fake_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72UIHGkfHizw"
      },
      "outputs": [],
      "source": [
        "#!pip install jovian --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEFq7FRWHi7d"
      },
      "outputs": [],
      "source": [
        "#import jovian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5nfodiWHjDR"
      },
      "outputs": [],
      "source": [
        "#jovian.commit(project=\"project-GANs for Art\", environment=None) \n",
        "#HELP? Visit - Learn more: https://jovian.ai/docs/user-guide/run.html#run-on-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC8dqTfMHjMA"
      },
      "outputs": [],
      "source": [
        "#jovian.reset()\n",
        "#jovian.log_hyperparams(lr=lr, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBTKWRakHjT3"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "epochs = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBP9r8Z9Hjar"
      },
      "outputs": [],
      "source": [
        "history = fit(epochs,lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkjj6GDgH83b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9nwaP8CH8-_"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0013.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK2-PRZqH9F9"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0037.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZNdwFQiH9Mi"
      },
      "outputs": [],
      "source": [
        "losses_g, losses_d, real_scores, fake_scores = history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikGdnIvTH9Ti"
      },
      "outputs": [],
      "source": [
        "jovian.log_metrics(loss_g=losses_g[-1], \n",
        "                   loss_d=losses_d[-1], \n",
        "                   real_score=real_scores[-1], \n",
        "                   fake_score=fake_scores[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq9EzclyH9br"
      },
      "outputs": [],
      "source": [
        "# Save the model checkpoints \n",
        "torch.save(generator.state_dict(), 'G.ckpt')\n",
        "torch.save(discriminator.state_dict(), 'D.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djGEHXkuH9iE"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses_d, '-')\n",
        "plt.plot(losses_g, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Discriminator', 'Generator'])\n",
        "plt.title('Losses');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bttL17d9H9pv"
      },
      "outputs": [],
      "source": [
        "plt.plot(real_scores, '-')\n",
        "plt.plot(fake_scores, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "plt.legend(['Real', 'Fake'])\n",
        "plt.title('Scores');"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1ufOdfPndK87xvHqKJPiG8Z9eAWQjZDfT",
      "authorship_tag": "ABX9TyNMlnY3gw0GlQxH/YbHUlBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}